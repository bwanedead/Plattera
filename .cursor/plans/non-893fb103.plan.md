<!-- 893fb103-721d-4e32-af3c-91b50d03b4f9 740c68a3-601b-4188-a432-47fb67ba967f -->
# Robust, observable deletion architecture (job-based)

## Core idea

Move single/bulk deletions to a background job model. The API returns immediately with a job_id; server executes purges in a bounded threadpool and publishes progress via SSE (with heartbeats) and a pollable status API. The frontend drives the modal off these signals (SSE first, polling fallback). This removes long-running requests and retry-induced stalls.

## Backend

1) Background deletion job manager

- New: `backend/services/dossier/delete_job_manager.py` managing jobs in-memory (with optional JSON snapshot under `dossiers_data/processing_jobs/deletion/` for resilience).
- APIs (in `backend/api/endpoints/dossier/dossier_management.py` or new `bulk_jobs.py`):
- `POST /api/dossier-management/bulk/start` → `{ job_id, total, acceptedIds, rejectedIds }`
- `GET /api/dossier-management/bulk/status/{job_id}` → `{ job_id, total, done, failedIds, inProgressIds, logs? }`
- `POST /api/dossier-management/bulk/cancel/{job_id}` (optional)
- Execution: bounded threadpool (e.g., 4 workers) using `run_in_threadpool`; per-dossier task is `DossierManagementService.delete_dossier(..., purge=True)`.
- Single delete path: optionally short-circuit to job manager with one id for consistent behavior.

2) SSE progress (job-scoped)

- New SSE: `GET /api/dossier/bulk/progress/{job_id}` streaming JSON events:
- `{"type":"bulk:start","job_id","total"}`
- `{"type":"dossier:deleted","job_id","dossier_id","ms"}`
- `{"type":"bulk:done","job_id","deleted","failed"}`
- Keep heartbeats (`event: ping`) every 10s. Reuse `event_bus` or a job-local queue.

3) Observability & idempotency

- Per-dossier timing logs: `BULK_PURGE_START|DONE|ERROR dossier=... ms=...`
- Final job summary: `BULK_JOB_SUMMARY job=... total=... deleted=... failed=... ms=...`
- Purge idempotency: treat missing dossier as not_found but non-fatal; always log `PURGE_SUMMARY` when purge function runs.

4) Stability

- Ensure backend never reloads in dev while jobs run (`reload=False` already). Tauri should invoke a single backend. Add a sanity log showing PID once.
- Optional: make deletion concurrency and queue size configurable via env (`DELETE_WORKERS`, `DELETE_QUEUE_MAX`).

## Frontend

5) Bulk flow → job start + progress

- Update `useDossierManager.bulkDelete`:
- POST `/bulk/start` with selected ids; receive `job_id` and `total`.
- Arm modal progress with `total`, set `done=0`.
- Connect SSE `/dossier/bulk/progress/{job_id}` and increment on `dossier:deleted` events.
- Fallback: poll `/bulk/status/{job_id}` every 1–2s if SSE drops; stop when `bulk:done` or `done===total`.
- Keep tombstones; on done, clear selection/search and reload first page.
- Single delete can still call `DELETE /{id}/delete`, or use the job path to unify UX; both will emit `dossier:deleted` SSE.

6) Network resilience

- FE requests for start/status use short timeouts (3–6s) and fast retries; the long work happens on the server, not the request.
- SSE uses heartbeats; if it disconnects, auto-reconnect to the same `job_id` or switch to polling.
- Optional dev proxy: add Next.js rewrite to route `/api` to `http://127.0.0.1:8000` to reduce localhost flaps.

## Logging & diagnostics

- On job start: log `BULK_JOB_START job=... total=... ids=[...]` (truncate or hash for length).
- Per dossier: `BULK_PURGE_START|DONE|ERROR` with durations; keep `PURGE_SUMMARY` from purge service.
- On job finish: `BULK_JOB_DONE job=... deleted=... failed=[...] duration_ms=...`.
- SSE lifecycle logs: subscribe/unsubscribe counts (already added).

## Migration strategy

- Keep current `POST /bulk` temporarily; behind the scenes, it can create a background job and return its `job_id` immediately.
- Gradually switch FE to the job-based endpoints. Remove the old long-running bulk route after verification.

## Validation

- Test 1/3/20 deletions, including heavy images; verify:
- Start returns quickly with `job_id`.
- Modal increments to total via SSE/polling.
- Backend logs show per-dossier timing and final summary.
- Kill SSE mid-run; confirm polling takes over.
- Start while API restarts; confirm FE reconnects and resumes.

## Deliverables

- New backend job manager, endpoints, and logs.
- Updated FE hook/modal using job progress with SSE+polling fallback.
- Next.js dev rewrite (optional) for stable localhost.
- Docs: README snippet on deletion job behavior, env flags, and logs.

### To-dos

- [ ] Implement deletion job manager (threadpool, in-memory store, optional snapshot)
- [ ] Add /bulk/start, /bulk/status, (/bulk/cancel) endpoints
- [ ] Add SSE /dossier/bulk/progress/{job_id} with heartbeats
- [ ] Add per-dossier timing and job summary logs
- [ ] Route single delete through job path or keep SSE publish
- [ ] Start job, drive modal from SSE; polling fallback
- [ ] Short timeouts for start/status; remove long bulk request
- [ ] Add Next.js rewrite to proxy /api to 127.0.0.1:8000 (dev)
- [ ] Manual test matrix: 1/3/20 deletions, SSE drop, restart